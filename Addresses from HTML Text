# =============================================
# ğŸ“Œ STEP 1 â€” Install dependencies
# =============================================
!pip install pandas requests tqdm

import pandas as pd
import requests
from tqdm import tqdm
from google.colab import files

# =============================================
# ğŸ“Œ STEP 2 â€” Upload CSV File
# =============================================
print("ğŸ“ Please upload the CSV file containing company link + extracted text...")
uploaded = files.upload()

csv_filename = list(uploaded.keys())[0]
df = pd.read_csv(csv_filename)

print("\nColumns found:", df.columns.tolist())

# ğŸ‘‰ Column Names (update if different)
LINK_COLUMN = "Company Link"
TEXT_COLUMN = "Extracted Text"

if TEXT_COLUMN not in df.columns:
    raise ValueError(f"Column '{TEXT_COLUMN}' not found in the CSV!")

if LINK_COLUMN not in df.columns:
    raise ValueError(f"Column '{LINK_COLUMN}' not found in the CSV!")


# =============================================
# ğŸ“Œ STEP 3 â€” DeepSeek API Configuration
# =============================================
DEEPSEEK_API_KEY = "Key"       # <-- replace with your API key
DEEPSEEK_URL = "https://api.deepseek.com/v1/chat/completions"

headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
}

# =============================================
# ğŸ“Œ STEP 4 â€” Extract addresses with labels
# =============================================
def extract_labeled_addresses(text):
    prompt = f"""
You are an AI that extracts ALL complete postal addresses from website content.
For each address, also extract the MOST LIKELY label based on context such as:

Head Office, Registered Office, Corporate Office, Branch Office,
Sales Office, Factory, Plant, Works, Warehouse, HO, RO, Marketing Office, Depot etc.

Return STRICTLY in JSON LIST format like this:

[
  {{"label": "Head Office", "address": "Plot 12, Industrial Area, Mumbai"}},
  {{"label": "Plant", "address": "GIDC Phase 2, Ahmedabad"}},
  {{"label": "Sales Office", "address": "MG Road, Delhi"}}
]

If no address is found, return: []
Do NOT add extra text outside the JSON.

TEXT:
{text}
"""

    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0
    }

    try:
        response = requests.post(DEEPSEEK_URL, headers=headers, json=payload, timeout=30)
        raw = response.json()["choices"][0]["message"]["content"].strip()

        # Try evaluating JSON-like output
        try:
            parsed = eval(raw)
            if isinstance(parsed, list):
                return parsed
        except:
            pass

        return []
    except Exception as e:
        return [{"label": "API Error", "address": str(e)}]


# =============================================
# ğŸ“Œ STEP 5 â€” Process Each Row
# =============================================
all_results = []

print("\nğŸ” Extracting labeled addresses using DeepSeek...\n")

text_list = df[TEXT_COLUMN].fillna("").astype(str).tolist()

for text in tqdm(text_list):
    extracted = extract_labeled_addresses(text)
    all_results.append(extracted)

df["Raw Address Data"] = all_results


# =============================================
# ğŸ“Œ STEP 6 â€” Expand into Address columns
# =============================================
max_count = max(len(r) for r in all_results)

print(f"\nğŸ“Œ Maximum labeled addresses found for any company: {max_count}\n")

for i in range(max_count):
    df[f"Address {i+1}"] = df["Raw Address Data"].apply(
        lambda arr: f"{arr[i]['label']}: {arr[i]['address']}" if i < len(arr) else ""
    )

df.drop(columns=["Raw Address Data"], inplace=True)


# =============================================
# ğŸ“Œ STEP 7 â€” Save Output CSV (Company Link + Address columns)
# =============================================
output_filename = "company_labeled_addresses_output.csv"
df.to_csv(output_filename, index=False)

print(f"\nâœ… DONE! Saved as: {output_filename}\n")

files.download(output_filename)

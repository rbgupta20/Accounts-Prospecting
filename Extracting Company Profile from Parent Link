# ==========================================
# STEP 1: Install dependencies
# ==========================================
!pip install requests beautifulsoup4 pandas lxml tldextract tqdm

# ==========================================
# STEP 2: Import libraries
# ==========================================
import requests
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urljoin, urlparse, urlunparse
import re
import tldextract
from tqdm import tqdm
from google.colab import files
from requests.exceptions import RequestException

# ==========================================
# STEP 3: Global headers
# ==========================================
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/120.0 Safari/537.36'
}

# ==========================================
# STEP 4: Helper - Extract visible text
# ==========================================
def extract_visible_text(html):
    soup = BeautifulSoup(html, 'lxml')
    for tag in soup(['script', 'style', 'noscript', 'header', 'footer', 'form', 'svg', 'nav']):
        tag.decompose()
    text = ' '.join(soup.stripped_strings)
    return re.sub(r'\s+', ' ', text)

# ==========================================
# STEP 5: Helper - Normalize URLs
# ==========================================
def normalize_url(url):
    parsed = urlparse(url)
    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))

# ==========================================
# STEP 6: Find likely About / Profile pages
# ==========================================
def find_about_page(base_url):
    try:
        res = requests.get(base_url, headers=HEADERS, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'lxml')
        links = soup.find_all('a', href=True)
        candidates = []
        for link in links:
            href = link['href'].lower()
            if any(kw in href for kw in ['about', 'profile', 'who-we-are', 'overview', 'company']):
                full_link = urljoin(base_url, href)
                candidates.append(normalize_url(full_link))
        if candidates:
            return list(set(candidates))
    except RequestException:
        pass
    return []

# ==========================================
# STEP 7: Extract "About Us" text
# ==========================================
def extract_about_text(url):
    try:
        res = requests.get(url, headers=HEADERS, timeout=15)
        res.raise_for_status()
        text = extract_visible_text(res.text)
        paragraphs = [p for p in text.split('.') if len(p.split()) > 5]
        about_text = '. '.join(paragraphs[:20])  # limit to first few sentences
        return about_text.strip()
    except RequestException:
        return ''

# ==========================================
# STEP 8: Main extraction function
# ==========================================
def extract_company_profiles():
    # Ask for CSV upload
    print("ðŸ“¤ Please upload your CSV file (must contain a column named 'URL')")
    uploaded = files.upload()
    csv_file = list(uploaded.keys())[0]
    df = pd.read_csv(csv_file)

    results = []

    for i, row in tqdm(df.iterrows(), total=len(df), desc="ðŸ” Extracting company data"):
        base_url = str(row['URL']).strip()
        if not base_url:
            continue
        if not base_url.startswith(('http://', 'https://')):
            base_url = 'https://' + base_url

        print(f"\nâž¡ï¸ Processing: {base_url}")
        company_name = tldextract.extract(base_url).domain.capitalize()

        # Find possible About pages
        about_pages = find_about_page(base_url)

        # Extract About text
        about_text = ''
        if about_pages:
            for page in about_pages:
                about_text = extract_about_text(page)
                if len(about_text) > 100:
                    break
        else:
            # fallback - extract from homepage
            about_text = extract_about_text(base_url)

        # Extract title as company name (if better)
        try:
            res = requests.get(base_url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(res.text, 'lxml')
            title = soup.title.string if soup.title else ''
            title_clean = re.sub(r'[-|â€¢].*', '', title).strip().title()
            if title_clean:
                company_name = title_clean
        except:
            pass

        results.append({
            'Company Name': company_name,
            'About Us / Company Profile': about_text,
            'Website': base_url
        })

    out_df = pd.DataFrame(results)
    out_df.to_csv('CompanyProfiles.csv', index=False)
    print("\nâœ… Extraction complete! File saved as CompanyProfiles.csv")

    files.download('CompanyProfiles.csv')

# ==========================================
# STEP 9: Run it
# ==========================================
extract_company_profiles()

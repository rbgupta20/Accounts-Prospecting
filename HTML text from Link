# =============================================
# ğŸ“¦ STEP 1: Install dependencies
# =============================================
!pip install pandas beautifulsoup4 requests lxml fake-useragent

# =============================================
# ğŸ“š STEP 2: Import libraries
# =============================================
import pandas as pd
import requests
from bs4 import BeautifulSoup
from google.colab import files
from fake_useragent import UserAgent
import time
import urllib3

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# =============================================
# ğŸŒ STEP 3: Smart Request Handler
# =============================================
ua = UserAgent()

def smart_get(url, retries=3):
    # âŒ Skip invalid URLs
    if not isinstance(url, str) or not url.strip():
        return "Error: Invalid or empty URL"

    url = url.strip()

    # Auto add https if missing
    if not url.startswith(("http://", "https://")):
        url = "https://" + url

    for attempt in range(retries):
        try:
            headers = {
                "User-Agent": ua.random,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
            }

            response = requests.get(
                url,
                timeout=12,
                headers=headers,
                allow_redirects=True,
                verify=False
            )

            if response.status_code == 200:
                return response.text

            if response.status_code in [403, 406, 429, 500, 502]:
                time.sleep(2 + attempt)
                continue

            return f"Error: HTTP {response.status_code}"

        except Exception as e:
            if attempt == retries - 1:
                return f"Error: {e}"
            time.sleep(1)

    return "Error: Max retries failed"


# =============================================
# ğŸ§¹ STEP 4: Extract Clean Text from HTML
# =============================================
def extract_text(url):
    html = smart_get(url)

    if html is None or str(html).startswith("Error"):
        return html

    try:
        soup = BeautifulSoup(html, "lxml")

        # Remove junk
        for tag in soup(["script", "style", "noscript", "iframe"]):
            tag.decompose()

        text = soup.get_text(" ", strip=True)
        return text

    except Exception as e:
        return f"Error parsing HTML: {e}"


# =============================================
# ğŸ“ STEP 5: Upload CSV
# =============================================
print("ğŸ“ Upload your CSV containing company website links")
uploaded = files.upload()

file_name = next(iter(uploaded))
df = pd.read_csv(file_name)

# Auto-detect link column
link_column = None
for col in df.columns:
    if "link" in col.lower() or "url" in col.lower():
        link_column = col
        break

if link_column is None:
    raise ValueError("âŒ No column containing 'link' or 'url' found")

print(f"ğŸ”— Using column: {link_column}")

# Clean data
df = df.dropna(subset=[link_column])
df[link_column] = df[link_column].astype(str)

# =============================================
# ğŸ”„ STEP 6: Process all links
# =============================================
results = []

for url in df[link_column]:
    print("â¡ Extracting:", url)

    try:
        text = extract_text(url)
    except Exception as e:
        text = f"Error: {e}"

    results.append({
        "Company Link": url,
        "Extracted Raw Text": text
    })

# =============================================
# ğŸ’¾ STEP 7: Save Output
# =============================================
output_df = pd.DataFrame(results)
output_path = "output_text_extracted.csv"
output_df.to_csv(output_path, index=False)

print("âœ… Extraction completed successfully!")
files.download(output_path)
